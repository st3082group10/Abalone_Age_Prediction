{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "l1fqEGSY8gXV",
        "outputId": "fa4c9d7c-7d2e-440d-a1f3-f1a877acc96d"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7ee86ea0-0531-4082-ba3c-d34e895a2173\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7ee86ea0-0531-4082-ba3c-d34e895a2173\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving abalone.csv to abalone.csv\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Sex  Length  Diameter  Height  Whole weight  Shucked weight  \\\n",
              "0  0.5   0.455     0.365   0.095        0.5140          0.2245   \n",
              "1  0.5   0.350     0.265   0.090        0.2255          0.0995   \n",
              "2  0.0   0.530     0.420   0.135        0.6770          0.2565   \n",
              "3  0.5   0.440     0.365   0.125        0.5160          0.2155   \n",
              "4  1.0   0.330     0.255   0.080        0.2050          0.0895   \n",
              "\n",
              "   Viscera weight  Shell weight  Rings  \n",
              "0          0.1010         0.150     15  \n",
              "1          0.0485         0.070      7  \n",
              "2          0.1415         0.210      9  \n",
              "3          0.1140         0.155     10  \n",
              "4          0.0395         0.055      7  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f28478df-1ffe-4685-8c5e-c8e18df75566\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Length</th>\n",
              "      <th>Diameter</th>\n",
              "      <th>Height</th>\n",
              "      <th>Whole weight</th>\n",
              "      <th>Shucked weight</th>\n",
              "      <th>Viscera weight</th>\n",
              "      <th>Shell weight</th>\n",
              "      <th>Rings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.455</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.095</td>\n",
              "      <td>0.5140</td>\n",
              "      <td>0.2245</td>\n",
              "      <td>0.1010</td>\n",
              "      <td>0.150</td>\n",
              "      <td>15</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.350</td>\n",
              "      <td>0.265</td>\n",
              "      <td>0.090</td>\n",
              "      <td>0.2255</td>\n",
              "      <td>0.0995</td>\n",
              "      <td>0.0485</td>\n",
              "      <td>0.070</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.420</td>\n",
              "      <td>0.135</td>\n",
              "      <td>0.6770</td>\n",
              "      <td>0.2565</td>\n",
              "      <td>0.1415</td>\n",
              "      <td>0.210</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.440</td>\n",
              "      <td>0.365</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.5160</td>\n",
              "      <td>0.2155</td>\n",
              "      <td>0.1140</td>\n",
              "      <td>0.155</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.330</td>\n",
              "      <td>0.255</td>\n",
              "      <td>0.080</td>\n",
              "      <td>0.2050</td>\n",
              "      <td>0.0895</td>\n",
              "      <td>0.0395</td>\n",
              "      <td>0.055</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f28478df-1ffe-4685-8c5e-c8e18df75566')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f28478df-1ffe-4685-8c5e-c8e18df75566 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f28478df-1ffe-4685-8c5e-c8e18df75566');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "\n",
        "# Reading the cleaned numeric abolone data set\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        " \n",
        "# To remove the scientific notation from numpy arrays\n",
        "np.set_printoptions(suppress=True)\n",
        " \n",
        "\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "import io\n",
        " \n",
        "aboloneAgeDataNumeric = pd.read_csv(io.BytesIO(uploaded['abalone.csv']))\n",
        "aboloneAgeDataNumeric.head()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate Target Variable and Predictor Variables\n",
        "TargetVariable=['Rings']\n",
        "Predictors=['Sex', 'Length', 'Diameter', 'Height', 'Whole weight','Shucked weight','Viscera weight', 'Shell weight']\n",
        "\n",
        "X=aboloneAgeDataNumeric[Predictors].values\n",
        "y=aboloneAgeDataNumeric[TargetVariable].values\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "### Normalization of data ###\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "PredictorScaler=MinMaxScaler()\n",
        "TargetVarScaler=MinMaxScaler()\n",
        " \n",
        "# Storing the fit object for later reference\n",
        "PredictorScalerFit=PredictorScaler.fit(X)\n",
        "TargetVarScalerFit=TargetVarScaler.fit(y)\n",
        " \n",
        "# Generating the standardized values of X and y\n",
        "X=PredictorScalerFit.transform(X)\n",
        "y=TargetVarScalerFit.transform(y)\n",
        "\n",
        "# Set the random seed\n",
        "np.random.seed(123)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)\n",
        "\n",
        "# Quick sanity check with the shapes of Training and testing datasets\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U-9tY9_SAmSA",
        "outputId": "80d58cb8-5924-426b-c142-1a148b06d318"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3341, 8)\n",
            "(3341, 1)\n",
            "(836, 8)\n",
            "(836, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Installing required libraries\n",
        "!pip install tensorflow\n",
        "!pip install keras"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7IiVmJWCMm1",
        "outputId": "3cb6521a-c226-46c3-e5b5-04fc0a71a10a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.8)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.0.3 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.10/dist-packages (2.12.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the libraries\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# create ANN model\n",
        "model = Sequential()\n",
        "\n",
        "# Defining the Input layer and FIRST hidden layer, both are same!\n",
        "model.add(Dense(units=8, input_dim=8, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "# Defining the first hidden layer of the model\n",
        "# after the first layer we don't have to specify input_dim as keras configure it automatically\n",
        "model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "# Defining the second hidden layer of the model\n",
        "model.add(Dense(units=1, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "# Defining the Third hidden layer of the model\n",
        "model.add(Dense(units=7, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "# The output neuron is a single fully connected node \n",
        "# Since we will be predicting a single number\n",
        "model.add(Dense(1, kernel_initializer='normal'))\n",
        "\n",
        "# Compiling the model\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "# Fitting the ANN to the Training set\n",
        "model.fit(X_train, y_train ,batch_size = 50, epochs = 500, verbose=1)"
      ],
      "metadata": {
        "id": "Jb8gLI8PCZ9l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d72d51ba-46a5-4235-bc74-3fdd29741bae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "67/67 [==============================] - 1s 1ms/step - loss: 0.0915\n",
            "Epoch 2/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0363\n",
            "Epoch 3/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0113\n",
            "Epoch 4/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0106\n",
            "Epoch 5/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0099\n",
            "Epoch 6/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0094\n",
            "Epoch 7/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0091\n",
            "Epoch 8/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0089\n",
            "Epoch 9/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0089\n",
            "Epoch 10/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0088\n",
            "Epoch 11/500\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0087\n",
            "Epoch 12/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0087\n",
            "Epoch 13/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0086\n",
            "Epoch 14/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0086\n",
            "Epoch 15/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0085\n",
            "Epoch 16/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0084\n",
            "Epoch 17/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0083\n",
            "Epoch 18/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0082\n",
            "Epoch 19/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0082\n",
            "Epoch 20/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0081\n",
            "Epoch 21/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0079\n",
            "Epoch 22/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0078\n",
            "Epoch 23/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0076\n",
            "Epoch 24/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0075\n",
            "Epoch 25/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0073\n",
            "Epoch 26/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0072\n",
            "Epoch 27/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0071\n",
            "Epoch 28/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0069\n",
            "Epoch 29/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0068\n",
            "Epoch 30/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0067\n",
            "Epoch 31/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0067\n",
            "Epoch 32/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0067\n",
            "Epoch 33/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0066\n",
            "Epoch 34/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0066\n",
            "Epoch 35/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 36/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 37/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 38/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 39/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 40/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0066\n",
            "Epoch 41/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 42/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 43/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 44/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 45/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 46/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 47/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 48/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 49/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 50/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 51/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 52/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 53/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 54/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 55/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 56/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 57/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 58/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 59/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 60/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 61/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 62/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 63/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 64/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 65/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0065\n",
            "Epoch 66/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 67/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 68/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 69/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 70/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 71/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 72/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 73/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 74/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 75/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 76/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 77/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 78/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 79/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 80/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 81/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 82/500\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0064\n",
            "Epoch 83/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 84/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 85/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 86/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 87/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 88/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 89/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 90/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 91/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 92/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 93/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 94/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 95/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 96/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 97/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 98/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 99/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 100/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 101/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 102/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 103/500\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0065\n",
            "Epoch 104/500\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0064\n",
            "Epoch 105/500\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0064\n",
            "Epoch 106/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 107/500\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0064\n",
            "Epoch 108/500\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0064\n",
            "Epoch 109/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 110/500\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0065\n",
            "Epoch 111/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 112/500\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0065\n",
            "Epoch 113/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 114/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 115/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 116/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 117/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 118/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 119/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 120/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 121/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 122/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 123/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 124/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 125/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 126/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 127/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 128/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 129/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 130/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 131/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 132/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 133/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 134/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 135/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0065\n",
            "Epoch 136/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 137/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0065\n",
            "Epoch 138/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 139/500\n",
            "67/67 [==============================] - 0s 5ms/step - loss: 0.0064\n",
            "Epoch 140/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 141/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 142/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 143/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 144/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 145/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 146/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 147/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 148/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 149/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 150/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 151/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 152/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 153/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 154/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 155/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 156/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 157/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 158/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 159/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 160/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 161/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 162/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 163/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 164/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 165/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 166/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 167/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 168/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 169/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 170/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 171/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 172/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 173/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 174/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 175/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 176/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 177/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 178/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 179/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 180/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 181/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 182/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 183/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 184/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 185/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 186/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 187/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 188/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 189/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 190/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 191/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 192/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 193/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 194/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 195/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 196/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 197/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 198/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 199/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 200/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 201/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 202/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 203/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 204/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 205/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 206/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 207/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 208/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 209/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 210/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 211/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 212/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 213/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 214/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 215/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 216/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 217/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 218/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 219/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 220/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 221/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 222/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 223/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 224/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 225/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 226/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 227/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 228/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 229/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 230/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 231/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 232/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 233/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 234/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 235/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 236/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 237/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 238/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 239/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 240/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 241/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 242/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 243/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 244/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 245/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 246/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 247/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 248/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 249/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 250/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 251/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 252/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 253/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 254/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 255/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 256/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 257/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 258/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 259/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 260/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 261/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 262/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 263/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 264/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 265/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 266/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 267/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 268/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 269/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 270/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 271/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 272/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 273/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 274/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 275/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 276/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 277/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 278/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 279/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 280/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 281/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 282/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 283/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 284/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 285/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 286/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 287/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 288/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 289/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 290/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 291/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 292/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 293/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 294/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 295/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 296/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 297/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 298/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 299/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 300/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 301/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 302/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 303/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 304/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 305/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 306/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 307/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 308/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 309/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 310/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 311/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 312/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 313/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 314/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 315/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 316/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 317/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 318/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 319/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 320/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 321/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 322/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 323/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0064\n",
            "Epoch 324/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 325/500\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0063\n",
            "Epoch 326/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0064\n",
            "Epoch 327/500\n",
            "67/67 [==============================] - 0s 4ms/step - loss: 0.0064\n",
            "Epoch 328/500\n",
            "67/67 [==============================] - 0s 3ms/step - loss: 0.0063\n",
            "Epoch 329/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 330/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 331/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 332/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 333/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 334/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 335/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 336/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 337/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 338/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 339/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 340/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 341/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 342/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 343/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 344/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 345/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 346/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 347/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 348/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 349/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 350/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 351/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 352/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 353/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 354/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 355/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 356/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 357/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 358/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 359/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 360/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 361/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 362/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 363/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 364/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 365/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 366/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 367/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 368/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 369/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 370/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 371/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 372/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 373/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 374/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 375/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 376/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 377/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 378/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 379/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 380/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 381/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 382/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 383/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 384/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 385/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 386/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 387/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 388/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 389/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 390/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 391/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 392/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 393/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 394/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 395/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 396/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 397/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 398/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 399/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 400/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 401/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 402/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 403/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 404/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 405/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 406/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 407/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 408/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 409/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 410/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 411/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 412/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 413/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 414/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 415/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 416/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 417/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 418/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 419/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 420/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 421/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 422/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 423/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 424/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 425/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 426/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 427/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 428/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 429/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 430/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 431/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 432/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 433/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 434/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 435/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 436/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 437/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 438/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 439/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 440/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 441/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 442/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 443/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 444/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 445/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 446/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 447/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 448/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 449/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 450/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 451/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 452/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 453/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 454/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 455/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 456/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 457/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 458/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 459/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 460/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 461/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 462/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 463/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 464/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 465/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0062\n",
            "Epoch 466/500\n",
            "67/67 [==============================] - 0s 2ms/step - loss: 0.0063\n",
            "Epoch 467/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 468/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 469/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 470/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 471/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 472/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 473/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 474/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 475/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 476/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 477/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 478/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 479/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 480/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 481/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 482/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 483/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 484/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 485/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 486/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 487/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 488/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 489/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 490/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 491/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 492/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n",
            "Epoch 493/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 494/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 495/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 496/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 497/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 498/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 499/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0063\n",
            "Epoch 500/500\n",
            "67/67 [==============================] - 0s 1ms/step - loss: 0.0064\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f5150a05cc0>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "def FunctionFindBestParams(X_train, y_train, X_val, y_val, X_test, y_test):\n",
        "    \n",
        "    # Defining the list of hyper parameters to try\n",
        "    batch_size_list=[5, 10, 15]\n",
        "    epoch_list  =   [100, 200, 300, 400, 500]\n",
        "    \n",
        "    import pandas as pd\n",
        "    SearchResultsData=pd.DataFrame(columns=['TrialNumber', 'Parameters', 'Accuracy'])\n",
        "    \n",
        "    # initializing the trials\n",
        "    TrialNumber=0\n",
        "    for batch_size_trial in batch_size_list:\n",
        "        for epochs_trial in epoch_list:\n",
        "            TrialNumber+=1\n",
        "            # create ANN model\n",
        "            model = Sequential()\n",
        "            # Defining the first layer of the model\n",
        "            model.add(Dense(units=8, input_dim=8, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "            # Defining the First hidden layer of the model\n",
        "            model.add(Dense(units=5, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "            # Defining the second hidden layer of the model\n",
        "            model.add(Dense(units=1, kernel_initializer='normal'))\n",
        "\n",
        "            # Defining the Third hidden layer of the model\n",
        "            model.add(Dense(units=7, kernel_initializer='normal', activation='relu'))\n",
        "\n",
        "            # The output neuron is a single fully connected node \n",
        "            # Since we will be predicting a single number\n",
        "            model.add(Dense(1, kernel_initializer='normal'))\n",
        "\n",
        "            # Compiling the model\n",
        "            model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "            # Define early stopping criteria\n",
        "            early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=10)\n",
        "\n",
        "            # Fitting the ANN to the Training set and validating on the validation set\n",
        "            model.fit(X_train, y_train ,batch_size = batch_size_trial, epochs = epochs_trial, verbose=0, \n",
        "                      validation_data=(X_val, y_val), callbacks=[early_stop])\n",
        "\n",
        "            # Evaluate the model on the test set and calculate accuracy\n",
        "            MAPE = model.evaluate(X_test, y_test, verbose=0)\n",
        "            accuracy = (1 - MAPE) * 100\n",
        "            \n",
        "            # printing the results of the current iteration\n",
        "            print(TrialNumber, 'Parameters:','batch_size:', batch_size_trial,'-', 'epochs:',epochs_trial, 'Accuracy:', accuracy)\n",
        "            \n",
        "            SearchResultsData = pd.concat([SearchResultsData, pd.DataFrame(data=[[TrialNumber, str(batch_size_trial)+'-'+str(epochs_trial), accuracy]], \n",
        "                                              columns=['TrialNumber', 'Parameters', 'Accuracy'])], \n",
        "                             ignore_index=True)\n",
        "            \n",
        "    return(SearchResultsData)\n",
        "\n",
        "# Split data into train, validation, and test sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=1)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_val, y_val, test_size=0.5, random_state=1)\n",
        "\n",
        "# Calling the function\n",
        "ResultsData = FunctionFindBestParams(X_train, y_train, X_val, y_val, X_test, y_test)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkFDgSMNTeXM",
        "outputId": "8a045e10-513c-447a-8fd4-475749017293"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 80: early stopping\n",
            "1 Parameters: batch_size: 5 - epochs: 100 Accuracy: 99.39695596694946\n",
            "Epoch 65: early stopping\n",
            "2 Parameters: batch_size: 5 - epochs: 200 Accuracy: 99.40503286197782\n",
            "Epoch 35: early stopping\n",
            "3 Parameters: batch_size: 5 - epochs: 300 Accuracy: 99.33063173666596\n",
            "Epoch 86: early stopping\n",
            "4 Parameters: batch_size: 5 - epochs: 400 Accuracy: 99.4195593520999\n",
            "Epoch 80: early stopping\n",
            "5 Parameters: batch_size: 5 - epochs: 500 Accuracy: 99.4110784959048\n",
            "Epoch 84: early stopping\n",
            "6 Parameters: batch_size: 10 - epochs: 100 Accuracy: 99.39811881631613\n",
            "Epoch 114: early stopping\n",
            "7 Parameters: batch_size: 10 - epochs: 200 Accuracy: 99.3758188560605\n",
            "Epoch 83: early stopping\n",
            "8 Parameters: batch_size: 10 - epochs: 300 Accuracy: 99.40854744054377\n",
            "Epoch 84: early stopping\n",
            "9 Parameters: batch_size: 10 - epochs: 400 Accuracy: 99.40537321381271\n",
            "Epoch 166: early stopping\n",
            "10 Parameters: batch_size: 10 - epochs: 500 Accuracy: 99.42014245316386\n",
            "Epoch 13: early stopping\n",
            "11 Parameters: batch_size: 15 - epochs: 100 Accuracy: 98.74571934342384\n",
            "Epoch 80: early stopping\n",
            "12 Parameters: batch_size: 15 - epochs: 200 Accuracy: 99.41022801212966\n",
            "Epoch 110: early stopping\n",
            "13 Parameters: batch_size: 15 - epochs: 300 Accuracy: 99.41517682746053\n",
            "Epoch 15: early stopping\n",
            "14 Parameters: batch_size: 15 - epochs: 400 Accuracy: 98.7456114962697\n",
            "Epoch 71: early stopping\n",
            "15 Parameters: batch_size: 15 - epochs: 500 Accuracy: 99.35703803785145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%matplotlib inline\n",
        "ResultsData.plot(x='Parameters', y='Accuracy', figsize=(10,2), kind='line')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "dcz7Eu15KuIB",
        "outputId": "5fae0a7a-b748-4139-cb04-177a487f720a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='Parameters'>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x200 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fitting the ANN to the Training set\n",
        "model.fit(X_train, y_train ,batch_size = 5, epochs = 400, verbose=0)\n",
        "\n",
        "# Generating Predictions on testing data\n",
        "Predictions=model.predict(X_test)\n",
        "\n",
        "# Scaling the predicted rings data back to original rings scale\n",
        "Predictions=TargetVarScalerFit.inverse_transform(Predictions)\n",
        "\n",
        "# Scaling the y_test Price data back to original rings scale\n",
        "y_test_orig=TargetVarScalerFit.inverse_transform(y_test)\n",
        "\n",
        "# Scaling the test data back to original scale\n",
        "Test_Data=PredictorScalerFit.inverse_transform(X_test)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PPIaYVREyrhe",
        "outputId": "62a2e1cf-4683-42f6-bece-0ed162fbf082"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14/14 [==============================] - 0s 1ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TestingData=pd.DataFrame(data=Test_Data, columns=Predictors)\n",
        "TestingData['Rings']=y_test_orig\n",
        "\n",
        "Test_Data_round=np.round(Predictions)\n",
        "TestingData['PredictedRings']=Test_Data_round"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "41xL6cPwSIV4",
        "outputId": "b8b76f8c-5ee4-4f63-8508-e816e2e55bb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Sex  Length  Diameter  Height  Whole weight  Shucked weight  \\\n",
              "0  0.5   0.630     0.515   0.175        1.1955          0.4920   \n",
              "1  0.0   0.620     0.485   0.220        1.5110          0.5095   \n",
              "2  1.0   0.625     0.485   0.160        1.1500          0.5255   \n",
              "3  0.0   0.530     0.400   0.165        0.7720          0.2855   \n",
              "4  0.5   0.645     0.490   0.160        1.2510          0.5355   \n",
              "\n",
              "   Viscera weight  Shell weight  Rings  PredictedRings  \n",
              "0          0.2470        0.3700   11.0            13.0  \n",
              "1          0.2840        0.5100   17.0            17.0  \n",
              "2          0.2570        0.3315   11.0            12.0  \n",
              "3          0.1975        0.2300   12.0            13.0  \n",
              "4          0.3345        0.3165    9.0            11.0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-97f97eed-d3a2-4916-9d06-07ab77e4bfdf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Length</th>\n",
              "      <th>Diameter</th>\n",
              "      <th>Height</th>\n",
              "      <th>Whole weight</th>\n",
              "      <th>Shucked weight</th>\n",
              "      <th>Viscera weight</th>\n",
              "      <th>Shell weight</th>\n",
              "      <th>Rings</th>\n",
              "      <th>PredictedRings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.630</td>\n",
              "      <td>0.515</td>\n",
              "      <td>0.175</td>\n",
              "      <td>1.1955</td>\n",
              "      <td>0.4920</td>\n",
              "      <td>0.2470</td>\n",
              "      <td>0.3700</td>\n",
              "      <td>11.0</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.620</td>\n",
              "      <td>0.485</td>\n",
              "      <td>0.220</td>\n",
              "      <td>1.5110</td>\n",
              "      <td>0.5095</td>\n",
              "      <td>0.2840</td>\n",
              "      <td>0.5100</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.485</td>\n",
              "      <td>0.160</td>\n",
              "      <td>1.1500</td>\n",
              "      <td>0.5255</td>\n",
              "      <td>0.2570</td>\n",
              "      <td>0.3315</td>\n",
              "      <td>11.0</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0.7720</td>\n",
              "      <td>0.2855</td>\n",
              "      <td>0.1975</td>\n",
              "      <td>0.2300</td>\n",
              "      <td>12.0</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0.490</td>\n",
              "      <td>0.160</td>\n",
              "      <td>1.2510</td>\n",
              "      <td>0.5355</td>\n",
              "      <td>0.3345</td>\n",
              "      <td>0.3165</td>\n",
              "      <td>9.0</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-97f97eed-d3a2-4916-9d06-07ab77e4bfdf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-97f97eed-d3a2-4916-9d06-07ab77e4bfdf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-97f97eed-d3a2-4916-9d06-07ab77e4bfdf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Computing the absolute percent error\n",
        "APE=100*(abs(TestingData['Rings']-TestingData['PredictedRings'])/TestingData['Rings'])\n",
        "TestingData['APE']=APE\n",
        "\n",
        "print('The Accuracy of ANN model is:', 100-np.mean(APE))\n",
        "TestingData.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "EaDqLLZ_Sdwh",
        "outputId": "14eb7bb7-7195-4b3a-eed6-5d325281753d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Accuracy of ANN model is: 82.62514451168036\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Sex  Length  Diameter  Height  Whole weight  Shucked weight  \\\n",
              "0  0.5   0.630     0.515   0.175        1.1955          0.4920   \n",
              "1  0.0   0.620     0.485   0.220        1.5110          0.5095   \n",
              "2  1.0   0.625     0.485   0.160        1.1500          0.5255   \n",
              "3  0.0   0.530     0.400   0.165        0.7720          0.2855   \n",
              "4  0.5   0.645     0.490   0.160        1.2510          0.5355   \n",
              "\n",
              "   Viscera weight  Shell weight  Rings  PredictedRings        APE  \n",
              "0          0.2470        0.3700   11.0            13.0  18.181818  \n",
              "1          0.2840        0.5100   17.0            17.0   0.000000  \n",
              "2          0.2570        0.3315   11.0            12.0   9.090909  \n",
              "3          0.1975        0.2300   12.0            13.0   8.333333  \n",
              "4          0.3345        0.3165    9.0            11.0  22.222222  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f7551f42-df16-49ca-9af8-78099221dc56\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Length</th>\n",
              "      <th>Diameter</th>\n",
              "      <th>Height</th>\n",
              "      <th>Whole weight</th>\n",
              "      <th>Shucked weight</th>\n",
              "      <th>Viscera weight</th>\n",
              "      <th>Shell weight</th>\n",
              "      <th>Rings</th>\n",
              "      <th>PredictedRings</th>\n",
              "      <th>APE</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.630</td>\n",
              "      <td>0.515</td>\n",
              "      <td>0.175</td>\n",
              "      <td>1.1955</td>\n",
              "      <td>0.4920</td>\n",
              "      <td>0.2470</td>\n",
              "      <td>0.3700</td>\n",
              "      <td>11.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>18.181818</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.620</td>\n",
              "      <td>0.485</td>\n",
              "      <td>0.220</td>\n",
              "      <td>1.5110</td>\n",
              "      <td>0.5095</td>\n",
              "      <td>0.2840</td>\n",
              "      <td>0.5100</td>\n",
              "      <td>17.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.485</td>\n",
              "      <td>0.160</td>\n",
              "      <td>1.1500</td>\n",
              "      <td>0.5255</td>\n",
              "      <td>0.2570</td>\n",
              "      <td>0.3315</td>\n",
              "      <td>11.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>9.090909</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.530</td>\n",
              "      <td>0.400</td>\n",
              "      <td>0.165</td>\n",
              "      <td>0.7720</td>\n",
              "      <td>0.2855</td>\n",
              "      <td>0.1975</td>\n",
              "      <td>0.2300</td>\n",
              "      <td>12.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>8.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.5</td>\n",
              "      <td>0.645</td>\n",
              "      <td>0.490</td>\n",
              "      <td>0.160</td>\n",
              "      <td>1.2510</td>\n",
              "      <td>0.5355</td>\n",
              "      <td>0.3345</td>\n",
              "      <td>0.3165</td>\n",
              "      <td>9.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>22.222222</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f7551f42-df16-49ca-9af8-78099221dc56')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f7551f42-df16-49ca-9af8-78099221dc56 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f7551f42-df16-49ca-9af8-78099221dc56');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Compute the MSE for training data\n",
        "y_train_pred = model.predict(X_train)\n",
        "train_mse = mean_squared_error(y_train, y_train_pred)\n",
        "\n",
        "# Compute the MSE for testing data\n",
        "y_test_pred = model.predict(X_test)\n",
        "test_mse = mean_squared_error(y_test, y_test_pred)\n",
        "\n",
        "print(\"Training RMSE:\", np.sqrt(train_mse))\n",
        "print(\"Testing RMSE:\", np.sqrt(test_mse))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ti60WC5UTUJX",
        "outputId": "b4053d59-f8a9-4905-ba53-8d4125762d92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "105/105 [==============================] - 0s 855us/step\n",
            "14/14 [==============================] - 0s 1ms/step\n",
            "Training RMSE: 0.07603964895238446\n",
            "Testing RMSE: 0.07824938793527764\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZrFbBv0wT20k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}